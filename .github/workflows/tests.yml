name: CI

on:
  push:
    branches: [ main, dev ]
  pull_request:
    branches: [ main, dev ]

jobs:
  lint:
    name: Lint Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - uses: astral-sh/setup-uv@v4
        with:
          enable-cache: false

      - name: Install ruff
        run: |
          set +e
          uv pip install --system ruff
          EXIT_CODE=$?
          set -e
          if [ $EXIT_CODE -ne 0 ]; then
            echo "âš ï¸ Failed to install ruff, skipping lint check"
            exit 0
          fi

      - name: Run ruff check (non-blocking)
        run: |
          set +e
          ruff check .
          EXIT_CODE=$?
          set -e
          
          if [ $EXIT_CODE -ne 0 ]; then
            echo "âš ï¸ Linting issues found"
            echo "This is a warning only - linting doesn't block PRs"
          fi

  offline-tests:
    name: Offline Tests
    runs-on: ubuntu-latest

    permissions:
      contents: read
      issues: write
      artifact-metadata: read
      packages: read

    # Add timeout for the entire job
    timeout-minutes: 30

    strategy:
      matrix:
        python-version: ['3.12']
      fail-fast: false

    steps:
    - uses: actions/checkout@v6
      with:
        # Add retry for checkout
        fetch-depth: 0
        persist-credentials: false

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}

    - name: Display Python environment
      run: |
        echo "ğŸ Python version: $(python --version)"
        echo "ğŸ“ Python path: $(which python)"
        echo "ğŸ“¦ Pip version: $(pip --version)"
        echo "ğŸ” Python architecture: $(python -c 'import platform; print(platform.platform())')"

    - name: Verify Python Setup
      run: |
        echo "ğŸ Verifying Python setup..."
        python --version
        python -c "import sys; print(f'Python {sys.version} setup successful')"
        echo "âœ… Python setup verified"

    - name: Cache pip packages
      uses: actions/cache@v5
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt', '**/pyproject.toml', '**/.github/workflows/tests.yml') }}-v2
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Handle Network Failures Gracefully
      run: |
        echo "ğŸŒ Configuring network resilience..."
        # Set longer timeouts for pip operations
        pip config set global.timeout 600
        # Configure retries for network operations
        pip config set global.retries 3

    - name: Install dependencies
      run: |
        echo "ğŸ“¦ Installing dependencies with enhanced error handling..."
        python -m pip install --upgrade pip setuptools wheel
        pip install -e ".[test,api]" --verbose
        # Ensure langgraph checkpoint dependencies are explicitly installed
        pip install "langgraph>=1.0.0" "langgraph-checkpoint>=4.0.0" "langgraph-checkpoint-sqlite>=3.0.0"
        # Ensure coverage and test dependencies are available
        pip install pytest-cov>=4.0.0 pytest-xdist coverage[toml]
        echo "âœ… Dependencies installed successfully"

    - name: Verify Test Discovery
      run: |
        echo "ğŸ” Verifying test discovery before execution..."
        python -m pytest tests/ --collect-only -m offline --ignore=tests/ui -q
        echo "âœ… Test discovery completed successfully"

    - name: Run offline tests
      id: run-tests
      run: |
        echo "ğŸ§ª Running offline tests with enhanced configuration..."
        set +e  # Don't exit on error - we want to capture the exit code
        python -m pytest tests/ \
          -m offline \
          --ignore=tests/ui \
          -v \
          --tb=short \
          --junitxml=test-results.xml \
          --cov=lightrag \
          --cov-report=xml \
          --cov-report=json \
          --cov-report=html \
          --cov-fail-under=10
        TEST_EXIT_CODE=$?
        echo "Test exit code: $TEST_EXIT_CODE"
        echo "test_exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

        # Exit with test code
        exit $TEST_EXIT_CODE

    - name: Run offline tests (fallback without coverage)
      if: steps.run-tests.outputs.test_exit_code != '0'
      run: |
        echo "ğŸ§ª Tests failed, running without coverage for detailed diagnostics..."
        python -m pytest tests/ \
          -m offline \
          --ignore=tests/ui \
          -v \
          --tb=long \
          --no-cov \
          2>&1 | tee pytest-output.txt
        echo "Exit code: $?"

    - name: Verify Coverage Files
      if: always()
      run: |
        echo "ğŸ” Verifying coverage files were generated..."
        echo "ğŸ“ Current directory: $(pwd)"
        echo "ğŸ“ Files in current directory:"
        ls -la

        echo "ğŸ” Looking for coverage files..."
        if [[ -f "coverage.xml" ]]; then
          echo "âœ… coverage.xml found"
          echo "  Size: $(wc -c < coverage.xml) bytes"
        else
          echo "âŒ coverage.xml NOT found"
        fi

        if [[ -f "coverage.json" ]]; then
          echo "âœ… coverage.json found"
          echo "  Size: $(wc -c < coverage.json) bytes"
        else
          echo "âŒ coverage.json NOT found"
        fi

        # Check if test exit code was successful
        TEST_EXIT_CODE="${{ steps.run-tests.outputs.test_exit_code }}"
        echo "Test exit code from previous step: $TEST_EXIT_CODE"

        # If tests passed but coverage files missing, that's an issue
        if [[ "$TEST_EXIT_CODE" == "0" ]]; then
          if [[ ! -f "coverage.xml" || ! -f "coverage.json" ]]; then
            echo "âš ï¸ Tests passed but coverage files missing - this may be a configuration issue"
          fi
        fi

        # Only fail if tests actually ran (not a collection error)
        if [[ "$TEST_EXIT_CODE" != "0" && "$TEST_EXIT_CODE" != "" ]]; then
          echo "Tests failed with exit code: $TEST_EXIT_CODE"
          # Don't fail here if tests failed - that's already captured
          exit 0
        fi

        # Final check - must have coverage files
        if [[ -f "coverage.xml" && -f "coverage.json" ]]; then
          echo "âœ… Coverage files verified"
        else
          echo "âŒ Coverage files missing"
          exit 1
        fi

    - name: Upload test results
      if: always() && (hashFiles('test-results.xml') != '')
      uses: actions/upload-artifact@v6
      with:
        name: test-results-py${{ matrix.python-version }}
        path: |
          test-results.xml
          .pytest_cache/
        retention-days: 7
        if-no-files-found: warn

    - name: Upload coverage artifacts
      if: always() && (hashFiles('coverage.xml') != '' || hashFiles('coverage.json') != '')
      uses: actions/upload-artifact@v6
      with:
        name: coverage-reports-py${{ matrix.python-version }}
        path: |
          coverage.xml
          coverage.json
          htmlcov/
        retention-days: 30
        if-no-files-found: warn

    - name: Handle CI failure and create issue
      if: failure()
      env:
        GH_TOKEN: ${{ github.token }}
      run: |
        echo "ğŸš¨ Tests failed - starting GitHub issue creation process..."
        echo "ğŸ” Checking GitHub CLI authentication..."

        if ! gh auth status > /dev/null 2>&1; then
          echo "âŒ GitHub CLI authentication failed"
          exit 1
        fi

        echo "âœ… GitHub CLI authenticated successfully"

        # Set environment variables for diagnostic script
        export GITHUB_ACTIONS=true
        export WORKFLOW_NAME="Offline Unit Tests"
        export RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        export GITHUB_REPOSITORY="${{ github.repository }}"
        export GITHUB_REF_NAME="${{ github.ref_name }}"
        export GITHUB_SHA="${{ github.sha }}"
        export GH_TOKEN="${{ github.token }}"

        echo "ğŸ§ª Running diagnostic script with CI environment..."
        echo "ğŸ“Š Environment:"
        echo "  - Workflow: $WORKFLOW_NAME"
        echo "  - Run URL: $RUN_URL"
        echo "  - Branch: $GITHUB_REF_NAME"
        echo "  - Commit: $GITHUB_SHA"
        echo "  - Repository: $GITHUB_REPOSITORY"

        # Run diagnostic script (will create issue if tests fail)
        if ./scripts/ci-diagnostic.sh; then
          echo "âœ… Diagnostic script completed successfully"
        else
          echo "âš ï¸ Diagnostic script failed, creating basic fallback issue..."

          # Create basic issue as fallback
          ISSUE_TITLE="CI failed: Offline Unit Tests"
          ISSUE_BODY="## ğŸš¨ CI/CD Pipeline Failure

          **Workflow:** Offline Unit Tests
          **Run URL:** $RUN_URL
          **Branch:** $GITHUB_REF_NAME
          **Commit:** $GITHUB_SHA
          **Timestamp:** $(date -u +%Y-%m-%dT%H:%M:%SZ)

          ### ğŸ”— Links
          - [Workflow Run]($RUN_URL)
          - [Repository](https://github.com/$GITHUB_REPOSITORY)

          ---
          ğŸ¤– This issue was automatically created by CI test failure.
          Please review the test output above and address the failure."

          if gh issue create \
            --title "$ISSUE_TITLE" \
            --body "$ISSUE_BODY" \
            --label "ci-failure" 2>/dev/null || gh issue create \
            --title "$ISSUE_TITLE" \
            --body "$ISSUE_BODY"; then
            echo "âœ… Basic GitHub issue created successfully"
          else
            echo "âŒ Failed to create GitHub issue"
            exit 1
          fi
        fi
