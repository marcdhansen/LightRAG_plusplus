#!/usr/bin/env python3
"""
Query Comparison Tool for Debugging /query vs /query/data

This script helps identify where context passing fails by running the same query
through both endpoints and comparing the complete flow.

Usage:
    python debug_query_comparison.py "Who is researcher?"
"""

import asyncio
import json
import sys
import argparse
from typing import Dict, Any, Optional
import httpx
from pathlib import Path

# Add lightrag to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from lightrag.utils import set_verbose_debug, VERBOSE_DEBUG


class QueryComparator:
    """Compare /query vs /query/data endpoint behavior"""

    def __init__(self, base_url: str = "http://localhost:9621"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=60.0)

    async def close(self):
        """Close HTTP client"""
        await self.client.aclose()

    async def test_query_endpoint(
        self, query: str, mode: str = "naive"
    ) -> Dict[str, Any]:
        """Test the /query endpoint"""
        print(f"\nğŸ” Testing /query endpoint with: '{query}'")

        try:
            response = await self.client.post(
                f"{self.base_url}/query", json={"query": query, "mode": mode}
            )
            response.raise_for_status()
            result = response.json()

            print(f"âœ… /query Response Status: {result.get('success', 'unknown')}")
            print(f"ğŸ“ /query Content Length: {len(result.get('response', ''))} chars")
            print(f"ğŸ“„ /query Response Preview: {result.get('response', '')[:200]}...")

            return result

        except Exception as e:
            print(f"âŒ /query Error: {e}")
            return {"error": str(e), "endpoint": "/query"}

    async def test_query_data_endpoint(
        self, query: str, mode: str = "naive"
    ) -> Dict[str, Any]:
        """Test the /query/data endpoint"""
        print(f"\nğŸ” Testing /query/data endpoint with: '{query}'")

        try:
            response = await self.client.post(
                f"{self.base_url}/query/data", json={"query": query, "mode": mode}
            )
            response.raise_for_status()
            result = response.json()

            print(
                f"âœ… /query/data Response Status: {result.get('data', {}).get('status', 'unknown')}"
            )

            # Analyze context data
            data = result.get("data", {})
            entities = data.get("entities", [])
            relations = data.get("relations", [])
            chunks = data.get("chunks", [])

            print(f"ğŸ“Š Context Statistics:")
            print(f"  â€¢ Entities: {len(entities)}")
            print(f"  â€¢ Relations: {len(relations)}")
            print(f"  â€¢ Chunks: {len(chunks)}")

            if chunks:
                print(
                    f"ğŸ“„ Sample Chunk Content: {chunks[0].get('content', '')[:150]}..."
                )

            return result

        except Exception as e:
            print(f"âŒ /query/data Error: {e}")
            return {"error": str(e), "endpoint": "/query/data"}

    def analyze_comparison(self, query_result: Dict, data_result: Dict, query: str):
        """Analyze differences between the two endpoints"""
        print(f"\nğŸ“Š Comparison Analysis for: '{query}'")
        print("=" * 60)

        # Check for errors
        if "error" in query_result:
            print(f"âŒ /query endpoint failed: {query_result['error']}")
            return

        if "error" in data_result:
            print(f"âŒ /query/data endpoint failed: {data_result['error']}")
            return

        # Compare context availability
        data = data_result.get("data", {})
        context_status = data.get("status", "unknown")

        print(f"ğŸ“‹ Context Status from /query/data: {context_status}")

        if context_status == "failure":
            failure_reason = data.get("message", "Unknown reason")
            print(f"âš ï¸  Context Retrieval Failure: {failure_reason}")

            # This explains why /query has no context!
            print("\nğŸ’¡ DIAGNOSIS: /query/data shows empty context retrieval")
            print("   This means the LLM in /query receives no context to work with")
            print(
                "   The generic response indicates the LLM is working, but context is missing"
            )
            return

        # Analyze content richness
        query_content = query_result.get("response", "")
        entities_count = len(data.get("entities", []))
        relations_count = len(data.get("relations", []))
        chunks_count = len(data.get("chunks", []))

        print(f"\nğŸ“ˆ Content Analysis:")
        print(f"  â€¢ /query Response Length: {len(query_content)} chars")
        print(f"  â€¢ Retrieved Entities: {entities_count}")
        print(f"  â€¢ Retrieved Relations: {relations_count}")
        print(f"  â€¢ Retrieved Chunks: {chunks_count}")

        # Check for generic responses
        generic_indicators = [
            "I don't have",
            "I don't know",
            "I don't have information",
            "I'm not sure",
            "I cannot",
            "no specific information",
        ]

        is_generic = any(
            indicator.lower() in query_content.lower()
            for indicator in generic_indicators
        )

        if is_generic and (entities_count > 0 or chunks_count > 0):
            print(f"\nâš ï¸  CRITICAL ISSUE: Generic response despite available context!")
            print(f"   â€¢ LLM received context but gave generic answer")
            print(f"   â€¢ This indicates a prompt construction or LLM binding issue")
            return

        if not is_generic and (entities_count == 0 and chunks_count == 0):
            print(f"\nğŸ¤” Specific response but no context retrieved")
            print(f"   â€¢ LLM may be using general knowledge instead of RAG")
            return

        print(f"\nâœ… Response appears appropriate for available context")


async def main():
    """Main debugging function"""
    parser = argparse.ArgumentParser(description="Compare LightRAG query endpoints")
    parser.add_argument("query", help="Query to test")
    parser.add_argument(
        "--url", default="http://localhost:9621", help="LightRAG server URL"
    )
    parser.add_argument("--mode", default="naive", help="Query mode")
    parser.add_argument(
        "--verbose", action="store_true", help="Enable verbose debugging"
    )

    args = parser.parse_args()

    if args.verbose:
        set_verbose_debug(True)
        print("ğŸ”§ Verbose debugging enabled")

    # Test server availability
    print(f"ğŸŒ Connecting to LightRAG server: {args.url}")

    comparator = QueryComparator(args.url)

    try:
        # Test both endpoints
        data_result = await comparator.test_query_data_endpoint(args.query, args.mode)
        query_result = await comparator.test_query_endpoint(args.query, args.mode)

        # Analyze the comparison
        comparator.analyze_comparison(query_result, data_result, args.query)

    except KeyboardInterrupt:
        print("\nâš ï¸  Debugging interrupted by user")
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {e}")
    finally:
        await comparator.close()

    print(f"\nğŸ Query comparison complete")


if __name__ == "__main__":
    asyncio.run(main())
