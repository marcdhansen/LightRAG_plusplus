###########################
### Server Configuration
###########################
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='LightRAG Gemini'
WEBUI_DESCRIPTION="Simple and Fast Graph Based RAG System"

# Directories (Absolute paths for clarity)
WORKING_DIR=/Users/marchansen/antigravity_lightrag/LightRAG/rag_storage
INPUT_DIR=/Users/marchansen/antigravity_lightrag/LightRAG/inputs

###########################
### LLM Configuration
###########################
LLM_BINDING=ollama
LLM_BINDING_HOST=http://127.0.0.1:11434
LLM_TIMEOUT=9000

# Separated Model Roles (Phase 1b)
# Indexing/Extraction Model: Best balance for entity extraction
LLM_MODEL=qwen2.5-coder:1.5b
# Query Execution Model: Fast performance for user interaction
QUERY_LLM_MODEL=qwen2.5-coder:1.5b

###########################
### Embedding Configuration
###########################
EMBEDDING_BINDING=ollama
EMBEDDING_MODEL=nomic-embed-text:v1.5
EMBEDDING_DIM=768
EMBEDDING_BINDING_HOST=http://127.0.0.1:11434

###########################
### Reranking Configuration (Phase 4a)
###########################
# Disable reranking for experiment
RERANK_BINDING=null
RERANK_MODEL=
RERANK_BINDING_HOST=
MIN_RERANK_SCORE=0.0

###########################
### Storage Configuration
###########################
LIGHTRAG_KV_STORAGE=JsonKVStorage
LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage
LIGHTRAG_GRAPH_STORAGE=NetworkXStorage
LIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage

# Memgraph Connection (Phase 1b)
MEMGRAPH_URI=bolt://localhost:7687

###########################
### Extraction Optimization
###########################
# Using YAML-style extraction for better parsing on small LLMs
EXTRACTION_FORMAT=key_value

###########################
### ACE Framework (Phase 3)
###########################
ENABLE_ACE=true

###########################
### Langfuse Observability (Phase 2.3)
###########################
LANGFUSE_HOST=http://localhost:3000
LANGFUSE_ENABLE_TRACE=false
LANGFUSE_PUBLIC_KEY=pk-lf-lightrag
LANGFUSE_SECRET_KEY=sk-lf-lightrag

###########################
### Evaluation Configuration (Phase 2)
###########################
# Best performing local model for Ragas Judge
EVAL_LLM_MODEL=qwen2.5-coder:0.5b
EVAL_EMBEDDING_MODEL=nomic-embed-text:v1.5

###########################
### Other Configuration
###########################
SUMMARY_LANGUAGE=English
TIKTOKEN_MODEL_NAME=gpt-4o-mini

###########################
### RAGAS Evaluation Config
###########################
# LLM running on standard Ollama port, but accessed as OpenAI-compatible /v1
EVAL_LLM_BINDING_API_KEY=ollama
EVAL_LLM_BINDING_HOST=http://127.0.0.1:11434/v1
EVAL_LLM_MODEL=qwen2.5-coder:1.5b

# Embeddings using native Ollama support
EVAL_EMBEDDING_BINDING=ollama
EVAL_EMBEDDING_BINDING_HOST=http://127.0.0.1:11434
EVAL_EMBEDDING_MODEL=nomic-embed-text:v1.5

# Performance
MAX_ASYNC=1
EVAL_MAX_CONCURRENT=1
EVAL_QUERY_TOP_K=5

###########################
### Token Conservation (P1)
###########################
# Prompt compression level: none, mini, standard
# - none: Use full prompts (best quality, more tokens)
# - mini: Use compressed prompts (~50% token reduction)
# - standard: Balanced prompts (default)
PROMPT_COMPRESSION=standard

# Quality tradeoff: high, balanced, low
# - high: Full extraction with more thoroughness
# - balanced: Standard extraction
# - low: Faster extraction with fewer gleaning iterations
EXTRACTION_QUALITY=balanced

# Token budget for entity descriptions (0 = unlimited)
# Reduce this to limit description verbosity
MAX_ENTITY_DESC_TOKENS=100

# Enable/disable examples in extraction prompts
# Disabling reduces tokens but may impact quality
USE_EXAMPLES_IN_PROMPT=true

###########################
### OpenViking Configuration
###########################
# OpenAI API key for OpenViking embeddings and VLM
OPENAI_API_KEY=your-openai-api-key-here
