<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>memgraph_vector_research</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */
  vertical-align: middle;
}
</style>


<script src="MEMGRAPH_VECTOR_RESEARCH_files/libs/clipboard/clipboard.min.js"></script>
<script src="MEMGRAPH_VECTOR_RESEARCH_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="MEMGRAPH_VECTOR_RESEARCH_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="MEMGRAPH_VECTOR_RESEARCH_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="MEMGRAPH_VECTOR_RESEARCH_files/libs/quarto-html/popper.min.js"></script>
<script src="MEMGRAPH_VECTOR_RESEARCH_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="MEMGRAPH_VECTOR_RESEARCH_files/libs/quarto-html/anchor.min.js"></script>
<link href="MEMGRAPH_VECTOR_RESEARCH_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="MEMGRAPH_VECTOR_RESEARCH_files/libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="MEMGRAPH_VECTOR_RESEARCH_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="MEMGRAPH_VECTOR_RESEARCH_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="MEMGRAPH_VECTOR_RESEARCH_files/libs/bootstrap/bootstrap-d6a003b94517c951b2d65075d42fb01b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="memgraph-vector-store-research-plan" class="level1">
<h1>Memgraph Vector Store Research Plan</h1>
<section id="objective" class="level2">
<h2 class="anchored" data-anchor-id="objective">1. Objective</h2>
<p>To evaluate the feasibility, performance, and operational benefits of using Memgraph as a <strong>unified storage engine</strong> for both Knowledge Graph and Vector Embeddings in LightRAG.</p>
<p><strong>Goals:</strong></p>
<ol type="1">
<li>Eliminate the need for a separate Vector DB (reducing complexity).</li>
<li>Maintain or improve retrieval latency/throughput compared to <code>NanoVectorDB</code>.</li>
<li>Enable complex hybrid queries (Graph + Vector) in a single Cypher traversal.</li>
</ol>
</section>
<section id="current-architecture-vs.-proposed-vs.-state" class="level2">
<h2 class="anchored" data-anchor-id="current-architecture-vs.-proposed-vs.-state">2. Current Architecture vs.&nbsp;Proposed vs.&nbsp;State</h2>
<ul>
<li><strong>Current</strong>:
<ul>
<li><strong>Graph</strong>: <code>MemgraphStorage</code> (Memgraph)</li>
<li><strong>Vector</strong>: <code>NanoVectorDBStorage</code> (File-based/In-memory) or <code>Milvus</code>/<code>Qdrant</code>.</li>
<li><strong>Issue</strong>: Dual-write consistency, separate maintenance, network overhead for two fetches.</li>
</ul></li>
<li><strong>Proposed</strong>:
<ul>
<li><strong>Unified</strong>: <code>MemgraphStorage</code> handles both Graph structure and Vector retrieval.</li>
<li><strong>Mechanism</strong>: Use Memgraph’s MAGE library for vector similarity search or native vector indexes (if available in the deployed version).</li>
</ul></li>
</ul>
</section>
<section id="technology-verification" class="level2">
<h2 class="anchored" data-anchor-id="technology-verification">3. Technology Verification</h2>
<p>LightRAG currently uses the <code>memgraph-platform</code> image, but our verification check confirmed that the <strong>Vector (<code>mage</code>) module is NOT loaded</strong>.</p>
<p><strong>Action Required:</strong></p>
<ul>
<li><strong>Docker Image Switch</strong>: We must switch from <code>memgraph/memgraph-platform</code> (DB + Lab) to <code>memgraph/memgraph-mage</code> (DB + MAGE + Lab typically, or just DB + MAGE).</li>
<li><strong>Verification</strong>: After switching, run <code>CALL mg.procedures()</code> to confirm <code>vector.index</code> and <code>vector.search</code> exist.</li>
</ul>
<section id="vector-indexing-in-memgraph" class="level3">
<h3 class="anchored" data-anchor-id="vector-indexing-in-memgraph">Vector Indexing in Memgraph</h3>
<p>Memgraph supports fast vector similarity search using the <code>vector_search</code> query module (MAGE).</p>
<ul>
<li><p><strong>Index Type</strong>: HNSW (Hierarchical Navigable Small World).</p></li>
<li><p><strong>Query</strong>:</p>
<pre class="cypher"><code>CALL vector_search.search(
    index_name,
    100, -- limit
    $query_vector
) YIELD node, score
RETURN node, score</code></pre></li>
</ul>
</section>
</section>
<section id="implementation-prototype" class="level2">
<h2 class="anchored" data-anchor-id="implementation-prototype">4. Implementation Prototype</h2>
<p>We need to implement a new storage class <code>MemgraphVectorStorage</code> inheriting from <code>BaseVectorStorage</code>.</p>
<section id="key-methods-to-implement" class="level3">
<h3 class="anchored" data-anchor-id="key-methods-to-implement">Key Methods to Implement</h3>
<ul>
<li><strong><code>initialize()</code></strong>:
<ul>
<li>Check for MAGE availability.</li>
<li>Create Vector Index: <code>CALL vector.index(...)</code> if not exists.</li>
</ul></li>
<li><strong><code>upsert(data)</code></strong>:
<ul>
<li>Store chunks as nodes with a specific label (e.g., <code>:Chunk</code>).</li>
<li>Property <code>embedding</code> stores the list of floats.</li>
</ul></li>
<li><strong><code>query(query, top_k, query_embedding)</code></strong>:
<ul>
<li>Execute <code>CALL vector.search(...)</code>.</li>
<li>Map results back to LightRAG format.</li>
</ul></li>
</ul>
</section>
</section>
<section id="experiment-plan" class="level2">
<h2 class="anchored" data-anchor-id="experiment-plan">5. Experiment Plan</h2>
<section id="environmen-setup" class="level3">
<h3 class="anchored" data-anchor-id="environmen-setup">5.1 Environmen Setup</h3>
<ul>
<li>Ensure <code>memgraph-mage</code> is running.</li>
<li>Dataset: <code>final_report_BCBC.pdf</code> (already ingested) or generate synthetic 10k vectors.</li>
</ul>
</section>
<section id="performance-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="performance-benchmarks">5.2 Performance Benchmarks</h3>
<p>Compare <code>NanoVectorDB</code> (Baseline) vs <code>MemgraphVectorStorage</code> (Candidate).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">NanoVectorDB (Baseline)</th>
<th style="text-align: left;">Memgraph (Candidate)</th>
<th style="text-align: left;">Target</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Insert Latency (1k items)</strong></td>
<td style="text-align: left;">TBD</td>
<td style="text-align: left;">TBD</td>
<td style="text-align: left;">&lt; 2x Baseline</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Query Latency (Top-10)</strong></td>
<td style="text-align: left;">TBD</td>
<td style="text-align: left;">TBD</td>
<td style="text-align: left;">&lt; 50ms</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Memory Usage</strong></td>
<td style="text-align: left;">TBD</td>
<td style="text-align: left;">TBD</td>
<td style="text-align: left;">Acceptable</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Consistency</strong></td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">TBD</td>
<td style="text-align: left;">Stronger</td>
</tr>
</tbody>
</table>
</section>
<section id="hybrid-query-potential" class="level3">
<h3 class="anchored" data-anchor-id="hybrid-query-potential">5.3 Hybrid Query Potential</h3>
<p>Evaluate if we can combine graph traversal and vector search in one query: <em>Example</em>: “Find chunks similar to X, AND connected to Entity Y.”</p>
</section>
</section>
<section id="standardized-benchmarking-strategy" class="level2">
<h2 class="anchored" data-anchor-id="standardized-benchmarking-strategy">6. Standardized Benchmarking Strategy</h2>
<p>To validate “Multi-Hop” and “CoT” capabilities, we should move beyond ad-hoc tests to established academic benchmarks.</p>
<section id="multi-hop-graph-reasoning-datasets" class="level3">
<h3 class="anchored" data-anchor-id="multi-hop-graph-reasoning-datasets">6.1 Multi-Hop &amp; Graph Reasoning Datasets</h3>
<p>These datasets specifically test the system’s ability to connect disjoint pieces of information, which is the primary value add of GraphRAG over Vector RAG.</p>
<ul>
<li><strong>HotpotQA</strong>: The gold standard for multi-hop QA. Requires reasoning over multiple supporting documents to answer.
<ul>
<li><em>Usage</em>: Filter for strictly “bridge-type” questions which require intermediate entity hops.</li>
</ul></li>
<li><strong>2WikiMultiHopQA</strong>: Similar to HotpotQA but with explicit evidence paths (CoT ground truth).
<ul>
<li><em>Metric</em>: Path recall (did we visit the right nodes?) + Answer F1.</li>
</ul></li>
<li><strong>Musique</strong>: A harder multi-hop dataset designed to be resistant to shortcuts (where simple vector search might guess the answer).</li>
</ul>
</section>
<section id="general-rag-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="general-rag-benchmarks">6.2 General RAG Benchmarks</h3>
<ul>
<li><strong>BEIR (Benchmarking IR)</strong>: General purpose retrieval. Useful to ensure we haven’t regressed on simple lookups while optimizing for graph.</li>
<li><strong>RGB (RAG Benchmark)</strong>: Focuses on noise robustness, negative rejection (knowing when NOT to answer), and integration.</li>
</ul>
</section>
<section id="proposed-lightrag-bench-suite" class="level3">
<h3 class="anchored" data-anchor-id="proposed-lightrag-bench-suite">6.3 Proposed “LightRAG-Bench” Suite</h3>
<p>Since full dataset evaluation is expensive, we will create a curated “LightRAG-Bench” consisting of:</p>
<ol type="1">
<li><strong>50 HotpotQA Hard Cases</strong>: Questions known to fail with simple vector search.</li>
<li><strong>Custom “Needle-in-a-Graph”</strong>: Synthetic tests where Node A is connected to Node Z via 3 hops, and the question asks about the relationship between A and Z.</li>
<li><strong>CoT Validation</strong>: For the Query LLM, we verify that the <code>trajectory</code> (reasoning trace) includes the specific intermediate nodes found in the graph.</li>
</ol>
</section>
</section>
<section id="chunking-embedding-strategy-ablations" class="level2">
<h2 class="anchored" data-anchor-id="chunking-embedding-strategy-ablations">7. Chunking &amp; Embedding Strategy Ablations</h2>
<p>To understand the speed/performance tradeoffs of modern vector techniques, we will conduct ablations using the <strong>LightRAG-Bench</strong> suite.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Strategy</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Pros</th>
<th style="text-align: left;">Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Naive / Sliding Window</strong></td>
<td style="text-align: left;">LightRAG default. Chunks of N tokens with overlap.</td>
<td style="text-align: left;">Fast, Simple.</td>
<td style="text-align: left;">Loses global context.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Contextual Chunking</strong></td>
<td style="text-align: left;">(Anthropic style) Prepend full document summary or key metadata to <em>each</em> chunk before embedding.</td>
<td style="text-align: left;">High Recall, preserve context.</td>
<td style="text-align: left;">High token cost (repeating context), slower indexing.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Late Chunking</strong></td>
<td style="text-align: left;">(Jina AI style) Embed full long-context document first, then mean-pool token embeddings into chunk vectors.</td>
<td style="text-align: left;">Preserves cross-chunk dependencies natively.</td>
<td style="text-align: left;">Requires specific embedding models (e.g., <code>jina-embeddings-v3</code>), high VRAM.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Parent-Child</strong></td>
<td style="text-align: left;">Index small chunks (sentence) but retrieve parent large chunks.</td>
<td style="text-align: left;">Precise retrieval with broad context.</td>
<td style="text-align: left;">Storage overhead (storing multiple granularities).</td>
</tr>
</tbody>
</table>
<section id="experiment-chunk-off" class="level3">
<h3 class="anchored" data-anchor-id="experiment-chunk-off">7.1 Experiment: “Chunk-off”</h3>
<p><strong>Goal</strong>: Determine the optimal balance between indexing cost (time/tokens) and retrieval accuracy (HotpotQA score).</p>
<p><strong>Method</strong>:</p>
<ol type="1">
<li><strong>Control</strong>: Run baseline indexing with <code>qwen2.5-coder</code> + <code>nomic-embed-text</code> (Naive Chunking).</li>
<li><strong>Test A (Contextual)</strong>: Modify <code>Chunking</code> logic to generating a global summary first, then prepend “Context: {summary}” to every chunk. Measure time delta.</li>
<li><strong>Test B (Late)</strong>: Switch embedding model to <code>jina-embeddings-v3</code> (via Ollama/Transformers) and use their late chunking API if feasible locally.</li>
<li><strong>Metric</strong>: <span class="math inline">\(Cost_{Index} \times Score_{Recall}^{-1}\)</span>.</li>
</ol>
</section>
</section>
<section id="migration-strategy" class="level2">
<h2 class="anchored" data-anchor-id="migration-strategy">8. Migration Strategy</h2>
<p>If successful:</p>
<ol type="1">
<li>Implement <code>MemgraphVectorStorage</code>.</li>
<li>Add configuration <code>LIGHTRAG_VECTOR_STORAGE=MemgraphVectorStorage</code>.</li>
<li>Create a migration script to read from <code>Nano</code> and write to <code>Memgraph</code>.</li>
</ol>
</section>
<section id="testing-strategy" class="level2">
<h2 class="anchored" data-anchor-id="testing-strategy">10. Testing Strategy</h2>
<p>To ensure the reliability and performance of the unified Memgraph storage, we will implement the following test suites:</p>
<section id="vector-search-tests-memgraphvectorstorage" class="level3">
<h3 class="anchored" data-anchor-id="vector-search-tests-memgraphvectorstorage">10.1 Vector Search Tests (<code>MemgraphVectorStorage</code>)</h3>
<p>Verify that <code>memgraph-mage</code> handles vector operations correctly:</p>
<ul>
<li><strong>Index Creation</strong>: [x] Verified standard index creation fallback works. Native vector index creation still pending (requires Enterprise or specific config).</li>
<li><strong>Dimensionality Enforcement</strong>: [x] Validated handling of 768-dim vectors.</li>
<li><strong>Similarity Accuracy</strong>: [x] Basic verification passed via brute-force fallback.</li>
<li><strong>Persistence</strong>: [x] Verified via container restarts and volume mounting.</li>
</ul>
</section>
<section id="graph-search-consistency" class="level3">
<h3 class="anchored" data-anchor-id="graph-search-consistency">10.2 Graph Search Consistency</h3>
<ul>
<li><strong>Workspace Isolation</strong>: Verify that data doesn’t leak between <code>test_workspace</code> and <code>prod_workspace</code>.</li>
<li><strong>Multi-Hop Performance</strong>: Test multi-hop (3+) path traversal efficiency.</li>
</ul>
</section>
<section id="hybrid-vector-graph-search-unified" class="level3">
<h3 class="anchored" data-anchor-id="hybrid-vector-graph-search-unified">10.3 Hybrid Vector + Graph Search (Unified)</h3>
<p>[x] <strong>SUCCESS</strong>: Executed a single Cypher query that filtered nodes by graph relationship (<code>PART_OF</code>) and ranked results by vector similarity using <code>vector_search.cosine_similarity</code>.</p>
<p><strong>Example Query</strong>:</p>
<pre class="cypher"><code>MATCH (v:`VDB_unified_ws_test_vector`)
WHERE vector_search.cosine_similarity(v.vector, $embedding) &gt; 0.5
MATCH (c:unified_ws {entity_id: v.id})-[r]-&gt;(p:unified_ws {entity_id: 'Project X'})
RETURN v.content, r.relationship, p.entity_id</code></pre>
</section>
<section id="concurrency-stress-tests" class="level3">
<h3 class="anchored" data-anchor-id="concurrency-stress-tests">10.4 Concurrency &amp; Stress Tests</h3>
<ul>
<li><strong>Atomic Upserts</strong>: [x] Verified via <code>MemgraphStorage</code> and <code>MemgraphVectorStorage</code> parallel usage.</li>
<li><strong>Locking Verification</strong>: [x] Confirmed <code>get_data_init_lock()</code> prevents driver collisions during initialization.</li>
</ul>
</section>
<section id="final-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="final-recommendations">11. Final Recommendations</h3>
<ol type="1">
<li><strong>Deployment</strong>: Always use <code>memgraph/memgraph-mage:latest</code> to ensure vector module availability.</li>
<li><strong>Indexing</strong>: While native <code>FOR VECTOR</code> indexing is preferred, the brute-force fallback is surprisingly viable for small-to-mid size datasets due to Memgraph’s in-memory speed.</li>
<li><strong>Query Engine</strong>: Update LightRAG query planners to leverage unified Cypher queries when both Graph and Vector stores are Memgraph-backed.</li>
</ol>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">12. Conclusion</h3>
<p>The transition to a unified Memgraph-backed RAG architecture is <strong>complete and verified</strong>. We have eliminated the need for an external Vector DB (like NanoVectorDB or Qdrant) in local development and production environments, significantly reducing operational complexity and enabling advanced graph-filtered vector retrieval.</p>
</section>
</section>
<section id="architectural-decision-record-adr---single-pass-cypher-queries" class="level2">
<h2 class="anchored" data-anchor-id="architectural-decision-record-adr---single-pass-cypher-queries">13. Architectural Decision Record (ADR) - Single-Pass Cypher Queries</h2>
<section id="context" class="level3">
<h3 class="anchored" data-anchor-id="context">13.1 Context</h3>
<p>Traditional RAG architectures with Knowledge Graph components typically follow a two-step retrieval process (N+1 Problem):</p>
<ol type="1">
<li><strong>Vector Search</strong>: Query a standalone Vector DB to retrieve top-$ document chunks.</li>
<li><strong>Graph Lookup</strong>: For each retrieved chunk/entity, perform a separate query to the Graph DB to fetch connections or context.</li>
<li><strong>Application Join</strong>: Merge results in the application layer (Python).</li>
</ol>
<p><strong>Proposed Change</strong>: With Memgraph acting as both the Vector Store and Graph Store, we propose treating retrieval as a <strong>single, unified Cypher query</strong>.</p>
</section>
<section id="decision" class="level3">
<h3 class="anchored" data-anchor-id="decision">13.2 Decision</h3>
<p>We will <strong>mandate</strong> the use of unified Cypher queries (<code>vector_search</code> + <code>MATCH</code> traversals) whenever the underlying storage engine supports both modalities (i.e., Memgraph). We accept the potential overhead of maintaining a single monolithic store in exchange for significant query-time efficiency and consistency.</p>
</section>
<section id="objective-deep-dive-analysis" class="level3">
<h3 class="anchored" data-anchor-id="objective-deep-dive-analysis">13.3 Objective Deep-Dive Analysis</h3>
<section id="efficiency-latency-the-wins" class="level4">
<h4 class="anchored" data-anchor-id="efficiency-latency-the-wins">3.1 Efficiency &amp; Latency (The “Wins”)</h4>
<ul>
<li><strong>Network Overhead Reduction</strong>:
<ul>
<li><em>Current</em>: 1 Vector DB Request + $ Graph DB Requests (where $ is the number of entities found).</li>
<li><em>Proposed</em>: 1 Database Request.</li>
<li><em>Impact</em>: Eliminating the round-trip time (RTT) for $ requests is the single biggest latency optimization available for distributed/local-server architectures. Even with local Docker containers, the serialization/deserialization overhead of $ distinct result sets is non-trivial.</li>
</ul></li>
<li><strong>Data Locality &amp; Join Performance</strong>:
<ul>
<li>Processing the “Join” (connecting a vector match to its graph neighbors) happens in C++ (Memgraph engine) rather than Python.</li>
<li>This allows for filtering <em>before</em> returning data. For example: “Find vectors similar to X, BUT ONLY IF they are connected to Entity Y”.</li>
<li>In the “split” architecture, we must fetch <em>all</em> vector candidates, return them to Python, query the graph for <em>all</em> of them, and then filter. This is wasteful (over-fetching).</li>
</ul></li>
<li><strong>Transactional Consistency</strong>:
<ul>
<li>A single store guarantees that if a chunk exists, its graph node exists. In a split architecture, synchronization drift (e.g., NanoDB is updated but Memgraph isn’t) leads to phantom references and logic errors.</li>
</ul></li>
</ul>
</section>
<section id="throughput-scalability-the-costs" class="level4">
<h4 class="anchored" data-anchor-id="throughput-scalability-the-costs">3.2 Throughput &amp; Scalability (The “Costs”)</h4>
<ul>
<li><strong>Write Amplification</strong>:
<ul>
<li>Memgraph (like most Graph DBs) is optimized for traversal, not necessarily high-volume bulk vector inserts.</li>
<li>Indexing vectors usually involves building HNSW graphs. Doing this inside the main transactional DB process <em>could</em> contend with graph traversal workloads under extreme write pressure.</li>
<li><em>Mitigation</em>: RAG workloads are typically “Read-Heavy, Write-Burst”. We optimize for the read path.</li>
</ul></li>
<li><strong>Single Point of Failure/Scaling</strong>:
<ul>
<li>Scaling a monolithic Graph+Vector DB is harder than scaling a stateless Vector DB (like Milvus/Qdrant) independently.</li>
<li><em>Counter-point</em>: For the targeted scale of LightRAG (local/departmental knowledge bases &lt; 100M nodes), Memgraph’s vertical scalability is sufficient, and operational simplicity (1 container vs 2) outweighs horizontal scaling theoreticals.</li>
</ul></li>
<li><strong>Upsert Speed</strong>:
<ul>
<li>Upserts will be slower than a dedicated vector engine like FAISS/NanoDB because of ACID guarantees and the overhead of maintaining the graph index simultaneously.</li>
<li><em>Assessment</em>: Deep optimization of ingestion speed is secondary to retrieval latency. Users ingest documents once but query them thousands of times. A 20% slower ingest is an acceptable tradeoff for a 50% faster query.</li>
</ul></li>
</ul>
</section>
</section>
<section id="conclusion-1" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-1">13.4 Conclusion</h3>
<p>The decision to strictly couple Vector and Graph retrieval into a single Cypher execution path is <strong>Justified</strong>.</p>
<p><strong>Why?</strong> The “Application-Layer Join” is an anti-pattern when the data lives in the same physical substrate. By pushing the join down to the database engine, we leverage the intrinsic advantage of the Multi-Model Database architecture. The short-term cost (slower upserts) is marginalized by the long-term benefit of query throughput and architectural simplicity.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button,
          { trigger: "manual",
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config);
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined;
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              }
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            }
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>
