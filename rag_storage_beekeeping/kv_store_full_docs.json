{
  "doc-b3605d9b002cc66b8b98fbe9a59a43f2": {
    "content": "--- Page 3 ---\nQuery + LLM\nEntity Name:  Beekeeper\nEntity Type: PERSON\nDescription: A Beekeeper is \nan individual who produces ...\nOriginal Chunks ID: xxx\nSource: Honey Bee\nTarget: Industrial agriculture\nKeywords: Agriculture ...\nDescription:Honey Bees are \nnegatively impacted ...\nOriginal Chunks ID: xxx\n... BEEKEEPER‘spractices involve\nthe methods and strategies employed\nby beekeepers to manage bee\ncolonies and ensure their health and\nproductivity. A Beekeeper is an\nindividual who produces honey and\nother related products, playing a\ncrucial role in ……\nBeekeepers engage in various tasks,\nincluding observing bee behavior,\nmonitoring hive conditions,\npreventing pest infestations, and\nutilizing techniques to handle bees,\nsuch as using smoke to calm them ...\n Original Text\n-----Entities-----\n\"Beekeeper\",”A Beekeeper is an\nindividual who produces honey and\nother related products, playing a\ncrucial role in ......”\n-Relationships-\n\"Beekeeper\",“bee\",\"Beekeepers\nmanage bees but do not develop\nindividual relationships with them\ndue to the limited interaction time\nwith each hive.”\n----Contexts----\nBEEKEEPER's practices involve the\nmethods and strategies employed\nby beekeepers to manage ……\nGraph-based Text Indexing Dual-level Retrieval Paradigm\nEntity & Rel Extraction\nBeekeeper BeesObserve\nBeekeeper\nA beekeeper \nis an person \nwho…\nLLM Profiling\nBeekeeper beekeeper\nDeduplication\n…\nMatch\nIndex Graph \nused for Retrieval\nBeekeeper Honey Bee\n…Formers Hive\nLow-level Keys\nAgriculture Production\nEnvironmental Impact …\nHigh-level Keys\nEntities\nRelations\nRetrieved Content\nFigure 1: Overall architecture of the proposed LightRAG framework.\nincludes two key functionalities: i) Data Indexer φ(·): which involves building a specific data\nstructure ˆD based on the external database D. ii) Data Retriever ψ(·): The relevant documents are\nobtained by comparing the query against the indexed data, also denoted as “relevant documents”. By\nleveraging the information retrieved through ψ(·) along with the initial query q, the generative model\nG(·) efficiently produces high-quality, contextually relevant responses.\nIn this work, we target several key points essential for an efficient and effective Retrieval-Augmented\nGeneration (RAG) system which are elaborated below:\n• Comprehensive Information Retrieval: The indexing function φ(·) must be adept at extracting\nglobal information, as this is crucial for enhancing the model’s ability to answer queries effectively.\n• Efficient and Low-Cost Retrieval : The indexed data structure ˆD must enable rapid and cost-\nefficient retrieval to effectively handle a high volume of queries.\n• Fast Adaptation to Data Changes: The ability to swiftly and efficiently adjust the data structure\nto incorporate new information from the external knowledge base, is crucial for ensuring that the\nsystem remains current and relevant in an ever-changing information landscape.\n3 T HE LIGHT RAG A RCHITECTURE\n3.1 G RAPH -BASED TEXT INDEXING\nGraph-Enhanced Entity and Relationship Extraction . Our LightRAG enhances the retrieval\nsystem by segmenting documents into smaller, more manageable pieces. This strategy allows for\nquick identification and access to relevant information without analyzing entire documents. Next,\nwe leverage LLMs to identify and extract various entities (e.g., names, dates, locations, and events)\nalong with the relationships between them. The information collected through this process will be\nused to create a comprehensive knowledge graph that highlights the connections and insights across\nthe entire collection of documents. We formally represent this graph generation module as follows:\nˆD = (ˆV, ˆE) =Dedupe ◦ Prof(V, E), V, E = ∪Di∈DRecog(Di) (2)\nwhere ˆD represents the resulting knowledge graphs. To generate this data, we apply three main\nprocessing steps to the raw text documents Di. These steps utilize a LLM for text analysis and\nprocessing. Details about the prompt templates and specific settings for this part can be found in\nAppendix 7.3.2. The functions used in our graph-based text indexing paradigm are described as:\n• Extracting Entities and Relationships. R(·): This function prompts a LLM to identify entities\n(nodes) and their relationships (edges) within the text data. For instance, it can extract entities\nlike \"Cardiologists\" and \"Heart Disease,\" and relationships such as \"Cardiologists diagnose Heart\nDisease\" from the text: \"Cardiologists assess symptoms to identify potential heart issues.\" To\nimprove efficiency, the raw textD is segmented into multiple chunks Di.\n• LLM Profiling for Key-Value Pair Generation. P(·): We employ a LLM-empowered profiling\nfunction, P(·), to generate a text key-value pair (K, V) for each entity node in V and relation\nedge in E. Each index key is a word or short phrase that enables efficient retrieval, while the\ncorresponding value is a text paragraph summarizing relevant snippets from external data to aid in\ntext generation. Entities use their names as the sole index key, whereas relations may have multiple\nindex keys derived from LLM enhancements that include global themes from connected entities.\n• Deduplication to Optimize Graph Operations . D(·): Finally, we implement a deduplication\nfunction, D(·), that identifies and merges identical entities and relations from different segments of\n3\n--- Page 5 ---\n• (iii) Incorporating High-Order Relatedness. To enhance the query with higher-order relatedness,\nLightRAGfurther gathers neighboring nodes within the local subgraphs of the retrieved graph\nelements. This process involves the set {vi|vi ∈ V ∧(vi ∈ Nv ∨ vi ∈ Ne)}, where Nv and Ne\nrepresent the one-hop neighboring nodes of the retrieved nodes v and edges e, respectively.\nThis dual-level retrieval paradigm not only facilitates efficient retrieval of related entities and relations\nthrough keyword matching, but also enhances the comprehensiveness of results by integrating relevant\nstructural information from the constructed knowledge graph.\n3.3 R ETRIEVAL -AUGMENTED ANSWER GENERATION\nUtilization of Retrieved Information. Utilizing the retrieved information ψ(q; ˆD), our LightRAG\nemploys a general-purpose LLM to generate answers based on the collected data. This data comprises\nconcatenated values V from relevant entities and relations, produced by the profiling function P(·). It\nincludes names, descriptions of entities and relations, and excerpts from the original text.\nContext Integration and Answer Generation. By unifying the query with this multi-source text,\nthe LLM generates informative answers tailored to the user’s needs, ensuring alignment with the\nquery’s intent. This approach streamlines the answer generation process by integrating both context\nand query into the LLM model, as illustrated in detailed examples (Appendix 7.2).\n3.4 C OMPLEXITY ANALYSIS OF THE LIGHT RAG F RAMEWORK\nIn this section, we analyze the complexity of our proposed LightRAG framework, which can be\ndivided into two main parts. The first part is the graph-based Index phase. During this phase, we use\nthe large language model (LLM) to extract entities and relationships from each chunk of text. As\na result, the LLM needs to be called total tokens\nchunk size times. Importantly, there is no additional overhead\ninvolved in this process, making our approach highly efficient in managing updates to new text.\nThe second part of the process involves the graph-based retrieval phase. For each query, we first\nutilize the large language model (LLM) to generate relevant keywords. Similar to current Retrieval-\nAugmented Generation (RAG) systems Gao et al. (2023; 2022); Chan et al. (2024), our retrieval\nmechanism relies on vector-based search. However, instead of retrieving chunks as in conventional\nRAG, we concentrate on retrieving entities and relationships. This approach markedly reduces\nretrieval overhead compared to the community-based traversal method used in GraphRAG.\n4 E VALUATION\nWe conduct empirical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions:• (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated with LightRAG, as well as its adaptability to data changes?\n4.1 E XPERIMENTAL SETTINGS\nEvaluation Datasets. To conduct a comprehensive analysis of LightRAG, we selected four datasets\nfrom the UltraDomain benchmark (Qian et al., 2024). The UltraDomain data is sourced from 428\ncollege textbooks and encompasses 18 distinct domains, including agriculture, social sciences, and\nhumanities. From these, we chose the Agriculture, CS, Legal, and Mix datasets. Each dataset contains\nbetween 600,000 and 5,000,000 tokens, with detailed information provided in Table 4. Below is a\nspecific introduction to the four domains utilized in our experiments:\n• Agriculture: This domain focuses on agricultural practices, covering a range of topics including\nbeekeeping, hive management, crop production, and disease prevention.\n• CS: This domain focuses on computer science and encompasses key areas of data science and\nsoftware engineering. It particularly highlights machine learning and big data processing, featuring\ncontent on recommendation systems, classification algorithms, and real-time analytics using Spark.\n5",
    "file_path": "unknown_source",
    "create_time": 1769522619,
    "update_time": 1769522619,
    "_id": "doc-b3605d9b002cc66b8b98fbe9a59a43f2"
  }
}
